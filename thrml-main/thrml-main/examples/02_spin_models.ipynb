{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ffcbb1",
   "metadata": {},
   "source": [
    "# Spin Models in THRML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab41a49ca978980",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Probabilistic computers that sample from graphical models defined over binary random variables are most natural to build using transistors, and therefore are of elevated interest to Extropic. As such, we've built some tooling into THRML that is dedicated to sampling from these binary PGMs and training machine learning models based on them. This notebook will walk through this functionality and show you how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373a4165214cfbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We specifically consider spin-valued EBMs with polynomial interactions. These models implement the probability distribution,\n\n\n$$ P(x) \\propto e^{-\\mathcal{E}(x)}$$\n\n$$ \\mathcal{E}(x) = -\\beta \\left( \\sum_{i \\in S_1} W^{(1)}_i s_i + \\sum_{(i, j) \\in S_2} W^{(2)}_{i, j} s_i s_j +  \\sum_{(i, j, k) \\in S_3} W^{(3)}_{i, j, k} s_i s_j s_k  + \\dots \\right) $$\n\nHere, the $s_i \\in \\{-1, 1\\}$ are spin variables that couple with each other via the $W^{(k)}$, which are scalars that represent the strengths of $k^{th}$ order interactions. $S_k$ is the set of all interactions of order $k$.\n\nA model of this type that contains at most second-order interactions is called an Ising model or Boltzmann machine. Boltzmann machines are one of the original machine learning models, and their significance was recognized in 2024 with a Nobel prize in physics for John Hopfield and Geoffrey Hinton. \n\nGibbs sampling defines a simple procedure for sampling from this type of model that is very hardware friendly. In particular, the Gibbs sampling update rule corresponding to the above energy function is,\n\n$$ P(s_i = 1 | s_{nb(i)}) = \\sigma[2 \\gamma]$$\n\n$$ \\gamma = W^{(1)}_i +  \\sum_{j \\in S_2[i]} W^{(2)}_{i, j} s_j + \\sum_{(j, k) \\in S_3[i]} W^{(3)}_{i, j, k} s_j s_k + \\dots$$\n\nwhere $s_{nb(i)}$ are the spins that are neighbours of $s_i$, and  $S_k[i]$ is the members of $S_k$ that contain $i$.\n\nFrom the above equation, we see that we can implement the Gibbs sampling update rule for a spin-valued model by computing simple functions of the neighbour states, multiply-accumulating the results, and then using them to generate an appropriately biased random bit. This can be done very efficiently using mixed signal (analog + digital) hardware; we flesh out a way to do this using only transistors on a modern process [in our recent paper](denoising.paper).\n\nNow that we understand the significance of this type of model, let's see how they can be sampled from using some of the tools built in to THRML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d619fe4764903fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, some imports,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "\n",
    "import dwave_networkx\n",
    "import jax.numpy as jnp\n",
    "import jax.random\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a211e5dbbe3faf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:49:54.803072750Z",
     "start_time": "2025-08-21T21:49:54.801486215Z"
    }
   },
   "outputs": [],
   "source": [
    "from thrml.block_management import Block\n",
    "from thrml.block_sampling import sample_states, SamplingSchedule\n",
    "from thrml.models.discrete_ebm import SpinEBMFactor\n",
    "from thrml.models.ising import (\n",
    "    estimate_kl_grad,\n",
    "    hinton_init,\n",
    "    IsingEBM,\n",
    "    IsingSamplingProgram,\n",
    "    IsingTrainingSpec,\n",
    ")\n",
    "from thrml.pgm import SpinNode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0188e4a833e24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this example, we will implement a quadratic binary model (Ising model). We will use DWave's \"Pegasus\" graph topology to allow us to directly compare the speed of our GPU-based sampler to results obtained using other hardware accelerators,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179458a1d73ae75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:49:54.965500938Z",
     "start_time": "2025-08-21T21:49:54.895056059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7c2aea39ba30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the graph using DWave's code\n",
    "graph = dwave_networkx.pegasus_graph(14)\n",
    "coord_to_node = {coord: SpinNode() for coord in graph.nodes}\n",
    "nx.relabel_nodes(graph, coord_to_node, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636f2d8847777cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can define our model using the functionality exposed by `thrml.models.ising`. For the sake of this example, we will choose random values for the biases and weights $W^{(1)}$ and $W^{(2)}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5766a16be7183f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:49:55.955806038Z",
     "start_time": "2025-08-21T21:49:54.969511274Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes = list(graph.nodes)\n",
    "edges = list(graph.edges)\n",
    "\n",
    "seed = 4242\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "biases = jax.random.normal(subkey, (len(nodes),))\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "weights = jax.random.normal(subkey, (len(edges),))\n",
    "\n",
    "beta = jnp.array(1.0)\n",
    "\n",
    "model = IsingEBM(nodes, edges, biases, weights, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47de141e5e53ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The `IsingEBM` class is simply a thin frontend that takes in your weights and biases and produces an appropriate set of `SpinEBMFactor`s,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39904a83a179db94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:49:56.007224135Z",
     "start_time": "2025-08-21T21:49:55.956185886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[thrml.models.discrete_ebm.SpinEBMFactor,\n",
       " thrml.models.discrete_ebm.SpinEBMFactor]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.__class__ for x in model.factors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03241add84f1c9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's do some computation using our `IsingEBM`. Specifically, we are going to look at the tools THRML exposes for *training* this type of model in the context of machine learning. In machine learning, the variables in an EBM are often segmented into \"visible\" variables (x) and \"latent\" variables (z). The visible variables represent the data, and the latent variables serve to increase the expressivity of the model. Given these latent variables, our EBMs model of the data is,\n\n$$ P(x) \\propto \\sum_z e^{-\\mathcal{E}(x, z)}$$\n \nWhen training EBMs, one is often interested in minimizing the distributional distance between the EBM and some dataset. This can be done by iteratively updating the model parameters according to the gradient,\n\n$$ \\nabla_{\\theta} D(Q(x)|| P(x)) = \\mathbb{E}_Q \\left[ \\mathbb{E}_{P(z|x)} \\left[ \\nabla_{\\theta} \\mathcal{E}\\right] - \\mathbb{E}_{P(z, \\: x)} \\left[ \\nabla_{\\theta} \\mathcal{E}\\right] \\right]$$\n\n\nWhere $D(Q||P)$ indicates the *KL-divergence* between Q and P, which is a common measure of distributional ditance in machine learning. Each of the two terms in this gradient can be estimated by sampling from the EBM. The first term is estimated by clamping the data nodes to a member of the dataset and sampling the latents. The second is estimated by sampling both the data and latent variables. We can leverage THRML for both of these computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e8acb160f83e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, lets set up our block specifications for both the free and clamped sampling. First, lets choose some random subset of our nodes to represent the data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c96efa3967472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:49:56.054109527Z",
     "start_time": "2025-08-21T21:49:56.008277505Z"
    }
   },
   "outputs": [],
   "source": [
    "n_data = 500\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "data_inds = np.random.choice(len(graph.nodes), n_data, replace=False)\n",
    "data_nodes = [nodes[x] for x in data_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421dbf5b924c055",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, lets compute the minimum coloring for the unclamped term in our gradient estimator,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7974889232a77308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:11.448177853Z",
     "start_time": "2025-08-21T21:49:56.048501870Z"
    }
   },
   "outputs": [],
   "source": [
    "coloring = nx.coloring.greedy_color(graph, strategy=\"DSATUR\")\n",
    "n_colors = max(coloring.values()) + 1\n",
    "free_coloring = [[] for _ in range(n_colors)]\n",
    "# form color groups\n",
    "for node in graph.nodes:\n",
    "    free_coloring[coloring[node]].append(node)\n",
    "\n",
    "free_blocks = [Block(x) for x in free_coloring]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763033f5e74d5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "and the same for the clamped term,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2635fd302853ba95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:11.503306872Z",
     "start_time": "2025-08-21T21:50:11.488434047Z"
    }
   },
   "outputs": [],
   "source": [
    "# in this case we will just re-use the free coloring\n",
    "# you can always do this, but it might not be optimal\n",
    "\n",
    "# a graph without the data nodes\n",
    "graph_copy = graph.copy()\n",
    "graph_copy.remove_nodes_from(data_nodes)\n",
    "\n",
    "clamped_coloring = [[] for _ in range(n_colors)]\n",
    "for node in graph_copy.nodes:\n",
    "    clamped_coloring[coloring[node]].append(node)\n",
    "\n",
    "clamped_blocks = [Block(x) for x in clamped_coloring]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6e43be0320b2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We have now defined everything we need to calculate some gradients! We can set up a few more details and get to it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5a41ab0c86a749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:13.284716466Z",
     "start_time": "2025-08-21T21:50:11.504385681Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets define some random \"data\" to use for our example\n",
    "# in real life this could be encoded images, text, video etc\n",
    "data_batch_size = 50\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "data = jax.random.bernoulli(subkey, 0.5, (data_batch_size, len(data_nodes))).astype(jnp.bool)\n",
    "\n",
    "# we will use the same sampling schedule for both cases\n",
    "schedule = SamplingSchedule(5, 100, 5)\n",
    "\n",
    "# convenient wrapper for everything you need for training\n",
    "training_spec = IsingTrainingSpec(model, [Block(data_nodes)], [], clamped_blocks, free_blocks, schedule, schedule)\n",
    "\n",
    "# how many parallel sampling chains to run for each term\n",
    "n_chains_free = data_batch_size\n",
    "n_chains_clamped = 1\n",
    "\n",
    "# initial states for each sampling chain\n",
    "# THRML comes with simple code for implementing the hinton initialization, which is commonly used with boltzmann machines\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "init_state_free = hinton_init(subkey, model, free_blocks, (n_chains_free,))\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "init_state_clamped = hinton_init(subkey, model, clamped_blocks, (n_chains_clamped, data_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b775e17516b51d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:20.767449305Z",
     "start_time": "2025-08-21T21:50:13.285970486Z"
    }
   },
   "outputs": [],
   "source": [
    "# now for gradient estimation!\n",
    "# this function returns the gradient estimators for the weights and edges of our model, along with the moment data that was used to estimate them\n",
    "# the moment data is also returned in case you want to use it for something else in your training loop\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "weight_grads, bias_grads, clamped_moments, free_moments = estimate_kl_grad(\n",
    "    subkey,\n",
    "    training_spec,\n",
    "    nodes,  # the nodes for which to compute bias gradients\n",
    "    edges,  # the edges for which to compute weight gradients\n",
    "    [data],\n",
    "    [],\n",
    "    init_state_clamped,\n",
    "    init_state_free,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ad8bb58cf1b66",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This function simply returns vectors for the weight and bias grads,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78e453a4e8f8ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:20.826027683Z",
     "start_time": "2025-08-21T21:50:20.767682787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7848     -0.33560008 -0.148      ...  0.00640005 -0.15759999\n",
      " -0.01319999]\n",
      "[0.43279994 1.1767999  0.04360002 ... 0.01319999 0.18919998 0.14079998]\n"
     ]
    }
   ],
   "source": [
    "print(weight_grads)\n",
    "print(bias_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f71bb255b61baa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "which can be used to train your model using whatever outer loop code you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c47f6ccdcbc10",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Because THRML is written in jax, it runs sampling programs very efficiently on GPUs and is competitive with the state of the art for sampling from sparse Ising models. Let's demonstrate that with a simple benchmark,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f685e2d",
   "metadata": {},
   "source": [
    "!!! warning\n",
    "\n",
    "    The following requires 8x GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b732330e-52ff-4f6e-bd08-6538ba96d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.sharding import PartitionSpec as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f125724ed56c9906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:30.713718285Z",
     "start_time": "2025-08-21T21:50:20.827994286Z"
    }
   },
   "outputs": [],
   "source": [
    "mesh = jax.make_mesh((8,), (\"x\",))\n",
    "sharding = jax.sharding.NamedSharding(mesh, P(\"x\"))\n",
    "\n",
    "timing_program = IsingSamplingProgram(model, free_blocks, [])\n",
    "\n",
    "timing_chain_len = 100\n",
    "\n",
    "batch_sizes = [8, 80, 800, 8000, 64_000, 160_000, 320_000]\n",
    "times = []\n",
    "flips = []\n",
    "dofs = []\n",
    "\n",
    "schedule = SamplingSchedule(timing_chain_len, 1, 1)\n",
    "\n",
    "call_f = jax.jit(\n",
    "    jax.vmap(lambda k: sample_states(k, timing_program, schedule, [x[0] for x in init_state_free], [], [Block(nodes)]))\n",
    ")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    keys = jax.device_put(keys, sharding)\n",
    "    _ = jax.block_until_ready(call_f(keys))\n",
    "\n",
    "    start_time = time.time()\n",
    "    _ = jax.block_until_ready(call_f(keys))\n",
    "    stop_time = time.time()\n",
    "\n",
    "    times.append(stop_time - start_time)\n",
    "    flips.append(timing_chain_len * len(nodes) * batch_size)\n",
    "    dofs.append(batch_size * len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3929ffa544d300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:30.713981162Z",
     "start_time": "2025-08-21T21:50:30.713620694Z"
    }
   },
   "outputs": [],
   "source": [
    "flips_per_ns = [x / (y * 1e9) for x, y in zip(flips, times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6faf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "plt.title(\"Performance on 8xB200\")\n",
    "axs.plot(dofs, flips_per_ns)\n",
    "axs.set_xscale(\"log\")\n",
    "axs.set_xlabel(\"Parallel Degrees of Freedom\")\n",
    "axs.set_ylabel(\"Flips/ns\")\n",
    "plt.savefig(\"fps.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgwsij6ijp8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../fps.png\" width=\"400\" style=\"background:white; padding:10px; border-radius:8px;\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80028122e2db6146",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can compare your results to an FPGA implementation that bakes the sampling problem directly into hardware [here](https://arxiv.org/abs/2303.10728) (they get ~60 flips/ns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12e34beb364142",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that despite our focus on quadratic models here, THRML comes with the ability to support spin interactions of arbitrary order out of the box. This ability can be accessed via `thrml.models.discrete_ebm.SpinEBMFactor`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f184a8ea1041946e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:50:31.020451022Z",
     "start_time": "2025-08-21T21:50:30.910617509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpinEBMFactor(\n",
       "  node_groups=[\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode()))\n",
       "  ],\n",
       "  weights=f32[10],\n",
       "  spin_node_groups=[\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode())),\n",
       "    Block(nodes=(SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode(), SpinNode()))\n",
       "  ],\n",
       "  categorical_node_groups=[],\n",
       "  is_spin={thrml.pgm.SpinNode: True}\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this creates a cubic interaction s_1 * s_2 * s_3 between a subset of our nodes\n",
    "SpinEBMFactor([Block(nodes[:10]), Block(nodes[10:20]), Block(nodes[20:30])], jax.random.normal(key, (10,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949870ecc78f2a2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "That's about everything there is to know about binary EBMs in THRML! We hope you use these tools to help us gain a better understanding of how to most effectively use these powerful primitives in more advanced machine learning architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "591c6f2f872d065a4e9643c78186d30103ad07846137f2348d83da528974561e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}